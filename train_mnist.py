import os
import math
import torch
import torchvision
from torchvision import transforms
from torchvision.utils import save_image

from models import VAE
from utils import make_gif, plot_elbocurve


# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Build the data input pipeline
""" 
# this function might be helpful if you want to use binarized-MNIST in your experiments
# in this case, you shou set "transform=transforms.Compose([transforms.Lambda(binarize), transforms.ToTensor()])"
def binarize(greyscale_img):
    binary_img = greyscale_img.convert('1')
    return binary_img
"""
batch_size = 100
train_dataset = torchvision.datasets.MNIST(root='./data/MNIST', train=True, transform=transforms.ToTensor(), download=False)
test_dataset = torchvision.datasets.MNIST(root='./data/MNIST', train=False, transform=transforms.ToTensor(), download=False)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Build the model by instantiating the class "VAE"
input_size = 784
hidden_size = 500
latent_size = 20
model = VAE(input_size, hidden_size, latent_size).to(device)

def compute_elbo(x, reconst_x, mean, log_var):
    # ELBO(Evidence Lower Bound) is the objective of VAE, we train the model just to maximize the ELBO.
    
    reconst_error = -torch.nn.functional.binary_cross_entropy(reconst_x, x, reduction='sum')
    # see Appendix B from VAE paper: "Kingma and Welling. Auto-Encoding Variational Bayes. ICLR-2014."
    # -KL[q(z|x)||p(z)] = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
    elbo = (reconst_error - kl_divergence) / len(x)
    return elbo

# Select the optimizer
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Create a folder to store the experiment results if it doesn't exist
results_dir = 'results/MNIST'
if not os.path.exists(results_dir):
    os.makedirs(results_dir)

# Save samples generated by the model before training
counter = 0
noise = torch.randn(25, latent_size).to(device)
generated_imgs = model.decode(noise).view(-1, 1, 28, 28)
save_image(generated_imgs, os.path.join(results_dir, 'samples-0.png'), nrow=5)

# Start training
num_epochs = 200
train_elbo = []
test_elbo = []
for epoch in range(1, num_epochs + 1):
    
    # model.train()   # optional, only useful when your model includes BatchNorm layers or Dropout layers
    for batch_idx, (batch_x, _) in enumerate(train_loader):
        # foward pass
        batch_data = batch_x.to(device).view(-1, input_size)
        batch_mean, batch_logvar, reconst_batch = model(batch_data)
        aver_loss = -compute_elbo(batch_data, reconst_batch, batch_mean, batch_logvar)
        # backprop and optimize
        optimizer.zero_grad()
        aver_loss.backward()
        optimizer.step()
        # print the average loss on batches
        if (batch_idx + 1) % 100 == 0:
            print('Epoch {}/{}, Batch {}/{}, Aver_Loss: {:.2f}'.format(
                epoch, num_epochs, batch_idx + 1, math.ceil(len(train_dataset) / batch_size), aver_loss.item()))
            
        # save samples generated by the model at the early training
        if epoch == 1:
            if (batch_idx + 1) == 10 or (batch_idx + 1) == 50 or (batch_idx + 1) == 100 or (batch_idx + 1) == 300 or (batch_idx + 1) == 500:
                counter += 1
                generated_imgs = model.decode(noise).view(-1, 1, 28, 28)
                save_image(generated_imgs, os.path.join(results_dir, 'samples-{}.png'.format(counter)), nrow=5)

                
    # model.eval()   # optional, corresponding to "model.train()"            
    with torch.no_grad():
        # elbo_curve on training-set
        total_elbo = 0
        for batch_idx, (batch_x, _) in enumerate(train_loader):
            batch_data = batch_x.to(device).view(-1, input_size)
            batch_mean, batch_logvar, reconst_batch = model(batch_data)
            total_elbo += compute_elbo(batch_data, reconst_batch, batch_mean, batch_logvar).item()
        aver_elbo = total_elbo / (batch_idx + 1)
        train_elbo.append(aver_elbo)
        # elbo_curve on test-set
        total_elbo = 0
        for batch_idx, (batch_x, _) in enumerate(test_loader):
            batch_data = batch_x.to(device).view(-1, input_size)
            batch_mean, batch_logvar, reconst_batch = model(batch_data)
            total_elbo += compute_elbo(batch_data, reconst_batch, batch_mean, batch_logvar).item()
        aver_elbo = total_elbo / (batch_idx + 1)
        test_elbo.append(aver_elbo)
        # save samples generated by the model at different training stages
        if epoch == 2 or epoch == 3 or epoch == 5 or epoch == 8 or epoch == 12 or epoch == 20:
            counter += 1
            generated_imgs = model.decode(noise).view(-1, 1, 28, 28)
            save_image(generated_imgs, os.path.join(results_dir, 'samples-{}.png'.format(counter)), nrow=5)
        if epoch % 30 == 0:
            counter += 1
            generated_imgs = model.decode(noise).view(-1, 1, 28, 28)
            save_image(generated_imgs, os.path.join(results_dir, 'samples-{}.png'.format(counter)), nrow=5)


# Save the trained model's parameters
paras_dir = 'trained_parameters'
if not os.path.exists(paras_dir):
    os.makedirs(paras_dir)
torch.save(model.state_dict(), os.path.join(paras_dir, 'mnist_zdim{}.pkl'.format(latent_size)))

# Make a GIF using the samples generated by the model during training
make_gif(results_dir, counter + 1)

# Plot the elbo-curve on both the training set and the test set
plot_elbocurve(train_elbo, test_elbo, latent_size, results_dir)
